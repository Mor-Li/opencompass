#!/usr/bin/env python
"""
Script to compare prompts generated by Unable-to-Forget and OpenCompass frameworks.
This ensures that both frameworks generate identical prompts for the same configuration.
"""

import json
import random
import sys
import os

# Add both frameworks to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'Unable-to-Forget', 'core'))

def generate_unable_to_forget_prompt(source_dict, n_tracked_keys, n_tracked_updates, 
                                    random_update=1, prompt_updating="colon", len_item=None):
    """Generate prompt using Unable-to-Forget logic (simplified version)."""
    
    # Sample keys
    random.seed(42)
    categories = list(source_dict.keys())
    tracked_keys = random.sample(categories, min(n_tracked_keys, len(categories)))[:n_tracked_keys]
    
    # Initialize values
    initial_values = {}
    current_values = {}
    for key in tracked_keys:
        items = source_dict[key]
        if len(items) >= 2:
            value = random.choice(items)
            if len_item is not None:
                # Apply lengthening (cap-strip method)
                target_length = len_item
                if len(value) < target_length:
                    value_cap = value[0].upper() + value[1:] if len(value) > 0 else value
                    result = value_cap
                    while len(result) < target_length:
                        result += value_cap
                    value = result[:target_length]
            initial_values[key] = value
            current_values[key] = value
    
    # Format tracked keys string
    str_tracked_keys = ", ".join(tracked_keys)
    n_actually_tracked_keys = len(tracked_keys)
    
    # Generate instruction (Unable-to-Forget format)
    instruction = f"As my secretary, I need you to carefully read a text stream where the values of multiple keys are being continuously updated."
    instruction += f"The {n_actually_tracked_keys} keys to track include {str_tracked_keys}. I will ask you to identify the value of each key later."
    
    # Generate updates
    updates = []
    update_format = {
        "colon": lambda k, v: f"{k}: {v}\n",
        "equal": lambda k, v: f"{k} = {v}\n",
        "hyphen": lambda k, v: f"{k} - {v}\n",
    }[prompt_updating]
    
    for i in range(n_tracked_updates):
        if random_update:
            key = random.choice(tracked_keys)
        else:
            key = tracked_keys[i % len(tracked_keys)]
        
        # Get new value
        items = source_dict[key]
        available = [item for item in items if item != current_values[key]]
        if available:
            new_value = random.choice(available)
            if len_item is not None:
                # Apply lengthening
                target_length = len_item
                if len(new_value) < target_length:
                    value_cap = new_value[0].upper() + new_value[1:] if len(new_value) > 0 else new_value
                    result = value_cap
                    while len(result) < target_length:
                        result += value_cap
                    new_value = result[:target_length]
            current_values[key] = new_value
            updates.append(update_format(key, new_value))
    
    # Format update stream
    stream = "The text stream starts on the next line.\n " + "".join(updates).rstrip()
    
    # Generate question (Unable-to-Forget format)
    probe_target = "current"
    question = f"What are the {probe_target} value of each key ({str_tracked_keys}) you are tracking? "
    question += f"End your response with: "
    question += f"'The {probe_target} value of <key> is <value>.'"
    question += f"Ensure that you report each key exactly once in this manner. "
    
    # Combine all parts
    full_prompt = f"{instruction}\n\n{stream}\n\n{question}"
    
    return full_prompt, current_values


def generate_opencompass_prompt(source_dict, n_tracked_keys, n_tracked_updates, 
                               random_update=1, prompt_updating="colon", len_item=None):
    """Generate prompt using OpenCompass PI-LLM logic (ALIGNED version)."""
    
    # Sample keys (using same seed)
    random.seed(42)
    categories = list(source_dict.keys())
    tracked_keys = random.sample(categories, min(n_tracked_keys, len(categories)))[:n_tracked_keys]
    
    # Initialize values
    initial_values = {}
    current_values = {}
    for key in tracked_keys:
        items = source_dict[key]
        if len(items) >= 2:
            value = random.choice(items)
            if len_item is not None:
                # Apply lengthening (cap-strip method)
                target_length = len_item
                if len(value) < target_length:
                    value_cap = value[0].upper() + value[1:] if len(value) > 0 else value
                    result = value_cap
                    while len(result) < target_length:
                        result += value_cap
                    value = result[:target_length]
            initial_values[key] = value
            current_values[key] = value
    
    # Build prompt in Unable-to-Forget format
    str_tracked_keys = ', '.join(tracked_keys)
    n_actually_tracked_keys = len(tracked_keys)
    
    # Instruction (UTF format)
    instruction = f"As my secretary, I need you to carefully read a text stream where the values of multiple keys are being continuously updated."
    instruction += f"The {n_actually_tracked_keys} keys to track include {str_tracked_keys}. I will ask you to identify the value of each key later."
    
    # Generate updates
    updates = []
    for i in range(n_tracked_updates):
        if random_update:
            key = random.choice(tracked_keys)
        else:
            key = tracked_keys[i % len(tracked_keys)]
        
        # Get new value
        items = source_dict[key]
        available = [item for item in items if item != current_values[key]]
        if available:
            new_value = random.choice(available)
            if len_item is not None:
                # Apply lengthening
                target_length = len_item
                if len(new_value) < target_length:
                    value_cap = new_value[0].upper() + new_value[1:] if len(new_value) > 0 else new_value
                    result = value_cap
                    while len(result) < target_length:
                        result += value_cap
                    new_value = result[:target_length]
            current_values[key] = new_value
            
            # Format update
            if prompt_updating == "colon":
                update_str = f"{key}: {new_value}"
            elif prompt_updating == "equal":
                update_str = f"{key} = {new_value}"
            else:
                update_str = f"{key}: {new_value}"
            updates.append(update_str)
    
    # Update stream (UTF format)
    if updates:
        update_stream = "The text stream starts on the next line.\n "
        formatted_updates = []
        for update in updates:
            formatted_updates.append(update + "\n")
        update_stream += "".join(formatted_updates).rstrip()
        instruction = f"{instruction}\n\n{update_stream}"
    
    # Question (UTF format)
    probe_target = "current"
    question = f"What are the {probe_target} value of each key ({str_tracked_keys}) you are tracking? "
    question += f"End your response with: "
    question += f"'The {probe_target} value of <key> is <value>.'"
    question += f"Ensure that you report each key exactly once in this manner. "
    
    # Full prompt
    full_prompt = f"{instruction}\n\n{question}"
    
    return full_prompt, current_values


def compare_prompts():
    """Compare prompts from both frameworks."""
    
    # Load test data
    data_path = 'data/pi_llm/dict_category_double-word_46-400_v1-1.json'
    with open(data_path, 'r') as f:
        source_dict = json.load(f)
    
    print("=" * 80)
    print("PROMPT ALIGNMENT VERIFICATION")
    print("=" * 80)
    
    # Test configurations
    test_configs = [
        {"name": "Test1 Basic", "n_keys": 5, "n_updates": 4, "random": 1, "format": "colon", "len_item": None},
        {"name": "Test2 More Keys", "n_keys": 10, "n_updates": 2, "random": 1, "format": "colon", "len_item": None},
        {"name": "Test3 Mixed", "n_keys": 23, "n_updates": 46, "random": 1, "format": "colon", "len_item": None},
        {"name": "Test4 Item Length", "n_keys": 5, "n_updates": 4, "random": 1, "format": "colon", "len_item": 10},
        {"name": "Test5 Sequential", "n_keys": 5, "n_updates": 8, "random": 0, "format": "colon", "len_item": None},
    ]
    
    for config in test_configs:
        print(f"\n{'='*60}")
        print(f"Testing: {config['name']}")
        print(f"Config: n_keys={config['n_keys']}, n_updates={config['n_updates']}, "
              f"random={config['random']}, format={config['format']}, len_item={config['len_item']}")
        print("-" * 60)
        
        # Generate prompts
        utf_prompt, utf_values = generate_unable_to_forget_prompt(
            source_dict, config['n_keys'], config['n_updates'], 
            config['random'], config['format'], config['len_item']
        )
        
        oc_prompt, oc_values = generate_opencompass_prompt(
            source_dict, config['n_keys'], config['n_updates'], 
            config['random'], config['format'], config['len_item']
        )
        
        # Compare prompts
        print("\nUnable-to-Forget Prompt:")
        print("-" * 40)
        print(utf_prompt[:500] + "..." if len(utf_prompt) > 500 else utf_prompt)
        
        print("\n\nOpenCompass Prompt:")
        print("-" * 40)
        print(oc_prompt[:500] + "..." if len(oc_prompt) > 500 else oc_prompt)
        
        print("\n\nDIFFERENCES FOUND:")
        print("-" * 40)
        
        # Check key differences
        if utf_prompt != oc_prompt:
            print("❌ PROMPTS ARE DIFFERENT!")
            
            # Analyze differences
            utf_lines = utf_prompt.split('\n')
            oc_lines = oc_prompt.split('\n')
            
            # Check instruction differences
            if "As my secretary" in utf_prompt and "Track the values" in oc_prompt:
                print("\n1. Instruction format differs:")
                print("   UTF: Uses 'As my secretary...' format")
                print("   OC:  Uses 'Track the values...' format")
            
            # Check initial values
            if "Initial values" in oc_prompt and "Initial values" not in utf_prompt:
                print("\n2. Initial values presentation:")
                print("   UTF: Does NOT show initial values")
                print("   OC:  Shows initial values explicitly")
            
            # Check update stream format
            if "The text stream starts on the next line" in utf_prompt:
                print("\n3. Update stream introduction:")
                print("   UTF: 'The text stream starts on the next line.'")
                print("   OC:  'Updates:' header")
            
            # Check question format
            print("\n4. Question format:")
            utf_q_start = utf_prompt.find("What are the")
            oc_q_start = oc_prompt.find("What are the")
            if utf_q_start >= 0:
                utf_question = utf_prompt[utf_q_start:]
                print(f"   UTF: {utf_question[:100]}...")
            if oc_q_start >= 0:
                oc_question = oc_prompt[oc_q_start:]
                print(f"   OC:  {oc_question[:100]}...")
        else:
            print("✅ Prompts are identical!")
        
        # Verify final values match
        print("\n\nFinal Values Check:")
        if utf_values == oc_values:
            print("✅ Final tracked values match!")
        else:
            print("❌ Final tracked values differ!")
            print(f"UTF: {utf_values}")
            print(f"OC:  {oc_values}")


if __name__ == "__main__":
    os.chdir(os.path.dirname(os.path.abspath(__file__)))
    compare_prompts()